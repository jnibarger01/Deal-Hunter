name: Database Backup

on:
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:

jobs:
  backup:
    name: Backup PostgreSQL Database
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Create database backup
        uses: appleboy/ssh-action@v1.0.0
        with:
          host: ${{ secrets.PRODUCTION_HOST }}
          username: ${{ secrets.PRODUCTION_USER }}
          key: ${{ secrets.PRODUCTION_SSH_KEY }}
          script: |
            # Create backup directory
            mkdir -p /opt/backups/dealhunter

            # Create backup with timestamp
            TIMESTAMP=$(date +%Y%m%d_%H%M%S)
            BACKUP_FILE="/opt/backups/dealhunter/backup_${TIMESTAMP}.sql"

            # Run pg_dump inside Docker container
            docker exec dealhunter-postgres-prod pg_dump -U dealhunter dealhunter > $BACKUP_FILE

            # Compress backup
            gzip $BACKUP_FILE

            # Remove backups older than 30 days
            find /opt/backups/dealhunter -name "*.sql.gz" -mtime +30 -delete

            echo "Backup completed: ${BACKUP_FILE}.gz"

      - name: Upload backup to S3 (optional)
        if: ${{ secrets.AWS_ACCESS_KEY_ID != '' }}
        uses: appleboy/ssh-action@v1.0.0
        with:
          host: ${{ secrets.PRODUCTION_HOST }}
          username: ${{ secrets.PRODUCTION_USER }}
          key: ${{ secrets.PRODUCTION_SSH_KEY }}
          script: |
            TIMESTAMP=$(date +%Y%m%d_%H%M%S)
            aws s3 cp /opt/backups/dealhunter/backup_${TIMESTAMP}.sql.gz \
              s3://${{ secrets.AWS_BACKUP_BUCKET }}/dealhunter/ \
              --region ${{ secrets.AWS_REGION }}
        continue-on-error: true

      - name: Notify backup status
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          text: 'Database backup ${{ job.status }}!'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
        continue-on-error: true
